{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python377jvsc74a57bd098b0a9b7b4eaaa670588a142fd0a9b87eaafe866f1db4228be72b4211d12040f",
   "display_name": "Python 3.7.7 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "/opt/ml/code\n"
     ]
    }
   ],
   "source": [
    "cd /opt/ml/code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "pytorch version: 1.4.0\nGPU 사용 가능 여부: True\nTesla P40\n1\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import json\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import segmentation_models_pytorch as smp\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from utils_modified import label_accuracy_score, add_hist\n",
    "\n",
    "import cv2\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pycocotools.coco import COCO\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "print(f'pytorch version: {torch.__version__}')\n",
    "print(f'GPU 사용 가능 여부: {torch.cuda.is_available()}')\n",
    "print(torch.cuda.get_device_name(0))\n",
    "print(torch.cuda.device_count())\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(random_seed=21):\n",
    "    torch.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed(random_seed)\n",
    "    #torch.cuda.manual_seed_all(random_seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(random_seed)\n",
    "    random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EDA():\n",
    "    with open(anns_file_path, 'r') as f:\n",
    "        dataset = json.loads(f.read())\n",
    "\n",
    "    categories = dataset['categories']\n",
    "    anns = dataset['annotations']\n",
    "    imgs = dataset['images']\n",
    "    nr_cats = len(categories)\n",
    "    nr_annotations = len(anns)\n",
    "    nr_images = len(imgs)\n",
    "\n",
    "    cat_names = []\n",
    "    super_cat_names = []\n",
    "    super_cat_ids = {}\n",
    "    super_cat_last_name = ''\n",
    "    nr_super_cats = 0\n",
    "    for cat_it in categories:\n",
    "        cat_names.append(cat_it['name'])\n",
    "        super_cat_name = cat_it['supercategory']\n",
    "        if super_cat_name != super_cat_last_name:\n",
    "            super_cat_names.append(super_cat_name)\n",
    "            super_cat_ids[super_cat_name] = nr_super_cats\n",
    "            nr_super_cats += 1\n",
    "    print('Number of super categories:', nr_super_cats)\n",
    "    print('Number of categories:', nr_cats)\n",
    "    print('Number of annotations:', nr_annotations)\n",
    "    print('Number of images:', nr_images)\n",
    "\n",
    "    cat_histogram = np.zeros(nr_cats, dtype=int)\n",
    "    for ann_it in anns:\n",
    "        cat_histogram[ann_it['category_id']] += 1\n",
    "\n",
    "    f, ax = plt.subplots(figsize=(5, 5))\n",
    "    df = pd.DataFrame({'Categories': cat_names, 'Number of annotations':cat_histogram})\n",
    "\n",
    "    plt.title('category distribution of train set')\n",
    "    sns.barplot(x='Number of annotations', y='Categories', data=df.sort_values('Number of annotations', ascending=False), label='Total', color='b')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data_dir, mode, transform):\n",
    "        super().__init__()\n",
    "        self.mode = mode\n",
    "        self.transform = transform\n",
    "        self.coco = COCO(data_dir)\n",
    "    \n",
    "    def __getitem__(self, index:int):\n",
    "        image_infos = self.coco.loadImgs(index)[0]\n",
    "\n",
    "        images = cv2.imread(os.path.join(dataset_path, image_infos['file_name']))\n",
    "        images = cv2.cvtColor(images, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "        #images /= 255.0\n",
    "\n",
    "        if self.mode in ('train', 'val'):\n",
    "            ann_ids = self.coco.getAnnIds(imgIds=image_infos['id'])\n",
    "            anns = self.coco.loadAnns(ann_ids)\n",
    "\n",
    "            masks = np.zeros((image_infos['height'], image_infos['width']))\n",
    "            for ann in anns:\n",
    "                masks = np.maximum(self.coco.annToMask(ann) * ann['category_id'], masks)\n",
    "            \n",
    "            images, masks = self.transform(image=images, mask=masks).values()\n",
    "            return images, masks, image_infos\n",
    "        \n",
    "        if self.mode == 'test':\n",
    "            images = self.transform(image=images)['image']\n",
    "            return images, image_infos\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.coco.getImgIds())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloader():\n",
    "    train_dataset = CustomDataset(data_dir=train_path, mode='train',transform=train_transform)\n",
    "    val_dataset = CustomDataset(data_dir=val_path, mode='val', transform=val_transform)\n",
    "    test_dataset = CustomDataset(data_dir=test_path, mode='test', transform=test_transform)\n",
    "\n",
    "    def collate_fn(batch):\n",
    "        return tuple(zip(*batch))\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                            batch_size=batch_size,\n",
    "                                            shuffle=True,\n",
    "                                            num_workers=4,\n",
    "                                            collate_fn=collate_fn,\n",
    "                                            drop_last=True)\n",
    "\n",
    "    val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                            batch_size=batch_size,\n",
    "                                            shuffle=False,\n",
    "                                            num_workers=4,\n",
    "                                            collate_fn=collate_fn)\n",
    "\n",
    "    test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                            batch_size=batch_size,\n",
    "                                            num_workers=4,\n",
    "                                            collate_fn=collate_fn)\n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_dataloader(dataloader):\n",
    "    data = iter(dataloader).next()\n",
    "    if len(data) == 3:\n",
    "        imgs, masks, image_infos = data\n",
    "        img = imgs[0]\n",
    "        mask = masks[0]\n",
    "        image_info = image_infos[0]\n",
    "\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(12, 12))\n",
    "        print('image shape:', list(img.shape))\n",
    "        print('mask shape:', list(mask.shape))\n",
    "        print('Unique values, category of transformed mask:\\n', {int(i):category_names[int(i)] for i in list(np.unique(mask))})\n",
    "\n",
    "        axes[0].imshow(img.permute([1, 2, 0]))\n",
    "        axes[0].grid(False)\n",
    "        axes[0].set_title('imput image:' + str(image_info['file_name']), fontsize=15)\n",
    "\n",
    "        axes[1].imshow(mask)\n",
    "        axes[1].grid(False)\n",
    "        axes[1].set_title('masks :' + str(image_info['file_name']), fontsize=15)\n",
    "\n",
    "        plt.show()\n",
    "    elif len(data) == 2:\n",
    "        imgs, image_infos = iter(dataloader).next()\n",
    "        img = imgs[0]\n",
    "        image_info = image_infos[0]\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(6, 6))\n",
    "        print('image shape:', list(img.shape))\n",
    "\n",
    "        ax.imshow(img.permute([1, 2, 0]))\n",
    "        ax.grid(False)\n",
    "        ax.set_title('imput image:' + str(image_info['file_name']), fontsize=15)\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(num_epochs, model, data_loader, val_loader, criterion, optimizer, saved_dir, val_every, device, print_log=True):\n",
    "    if print_log:\n",
    "        print('Start training')\n",
    "    best_mIoU = 0\n",
    "    hist_mIoU = []\n",
    "    epoch_begin = 0\n",
    "\n",
    "    for epoch in tqdm(range(0, num_epochs)):\n",
    "        model.train()\n",
    "        for step, (images, masks, _) in tqdm(enumerate(data_loader)):\n",
    "            images = torch.stack(images).to(device)\n",
    "            masks = torch.stack(masks).long().to(device)\n",
    "            outputs = model(images)\n",
    "            \n",
    "            loss = criterion(outputs, masks)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if print_log:\n",
    "                if (step + 1) % 25 == 0:\n",
    "                    print(f'Epoch[{epoch + 1}/{num_epochs}], Step[{step + 1}/{len(data_loader)}], Loss: {loss.item():.4f}')\n",
    "                    \n",
    "        if (epoch + 1) % val_every == 0:\n",
    "            mIoU = validation(epoch + 1, model, val_loader, criterion, device)\n",
    "            hist_mIoU.append(mIoU)\n",
    "            if mIoU > best_mIoU:\n",
    "                if print_log:\n",
    "                    print(f'Best performance at epoch: {epoch + 1}')\n",
    "                    print('Save model in', saved_dir)\n",
    "                best_mIoU = mIoU\n",
    "                save_model(model, saved_dir)\n",
    "\n",
    "    return hist_mIoU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(epoch, model, data_loader, criterion, device):\n",
    "    print(f'Start validation #{epoch}')\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        hist = np.zeros((N_CLASSES, N_CLASSES))\n",
    "        for step, (images, masks, _) in enumerate(data_loader):\n",
    "            images = torch.stack(images).to(device)\n",
    "            masks = torch.stack(masks).long().to(device)\n",
    "            outputs = model(images)\n",
    "\n",
    "            loss = criterion(outputs, masks)\n",
    "\n",
    "            outputs = torch.argmax(outputs.squeeze(), dim=1)\n",
    "            outputs = outputs.detach().cpu().numpy()\n",
    "            masks = masks.detach().cpu().numpy()\n",
    "            hist = add_hist(hist, masks, outputs, n_class=N_CLASSES)\n",
    "        \n",
    "        acc, acc_cls, mIoU, fwavacc = label_accuracy_score(hist)\n",
    "        print(f'Validation #{epoch} mIoU: {mIoU:.4f}')\n",
    "    return mIoU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, saved_dir, file_name=f'{model_name}.pt'):\n",
    "    os.makedirs(saved_dir, exist_ok=True)\n",
    "    check_point = {'net':model.state_dict()}\n",
    "    output_path = os.path.join(saved_dir, file_name)\n",
    "    torch.save(model.state_dict(), output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_name):\n",
    "    model_path = saved_path + f'/{model_name}.pt'\n",
    "    checkpoint = torch.load(model_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_eval_model(model, dataloader):\n",
    "    for imgs, image_infos in dataloader:\n",
    "        model.eval()\n",
    "        outs = model(torch.stack(imgs).to(device))\n",
    "        outs = torch.argmax(outs.squeeze(), dim=1).detach().cpu().numpy()\n",
    "        break\n",
    "\n",
    "    imgs = imgs[0]\n",
    "    image_infos = image_infos[0]\n",
    "    outs = outs[0]\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 16))\n",
    "    print('Shape of Original Image:', list(imgs.shape))\n",
    "    print('Shape of Predicted:', list(outs.shape))\n",
    "    print('Unique values, category of transformed mask\\n', {int(i):category_names[int(i)] for i in list(np.unique(outs))})\n",
    "\n",
    "    axes[0].imshow(imgs.permute([1,2,0]))\n",
    "    axes[0].grid(False)\n",
    "    axes[0].set_title('Original image:' + str(image_infos['file_name']), fontsize=15)\n",
    "\n",
    "    axes[1].imshow(outs)\n",
    "    axes[1].grid(False)\n",
    "    axes[1].set_title('Predicted:' + str(image_infos['file_name']), fontsize=15)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, data_loader, device):\n",
    "    size = 256\n",
    "    transform = A.Compose([A.Resize(256, 256)])\n",
    "    print('Start prediction')\n",
    "    model.eval()\n",
    "\n",
    "    file_name_list = []\n",
    "    preds_array = np.empty((0, size*size), dtype=np.long)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for step, (imgs, image_infos) in tqdm(enumerate(data_loader)):\n",
    "            outs = model(torch.stack(imgs).to(device))\n",
    "            outs = torch.argmax(outs.squeeze(), dim=1).detach().cpu().numpy()\n",
    "\n",
    "            masks = []\n",
    "            for img, mask in zip(np.stack(imgs), outs):\n",
    "                img, mask = transform(image=img, mask=mask).values()\n",
    "                masks.append(mask)\n",
    "            outs = np.array(masks)\n",
    "            outs = outs.reshape([outs.shape[0], size * size]).astype(int)\n",
    "            preds_array = np.vstack((preds_array, outs))\n",
    "\n",
    "            file_name_list.append([i['file_name'] for i in image_infos])\n",
    "    print('End prediction.')\n",
    "    file_names = [y for x in file_name_list for y in x]\n",
    "    return file_names, preds_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_submission(model, dataloader):\n",
    "    submission = pd.read_csv('./submission/sample_submission.csv', index_col=None)\n",
    "    file_names, preds = test(model, dataloader, device)\n",
    "\n",
    "    for file_name, string in zip(file_names, preds):\n",
    "        submission = submission.append({'image_id':file_name, 'PredictionString':' '.join(str(e) for e in string.tolist())}, ignore_index=True)\n",
    "\n",
    "    submission.to_csv(submission_path + f'/{model_name}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_CLASSES = 12\n",
    "batch_size = 8\n",
    "num_epochs = 8\n",
    "image_size = 256\n",
    "learning_rate = 0.0001\n",
    "model_name = 'model_test'\n",
    "random_seed = 21\n",
    "val_every = 1\n",
    "\n",
    "model = smp.DeepLabV3Plus(\n",
    "    encoder_name=\"resnext50_32x4d\",\n",
    "    encoder_weights=\"imagenet\",\n",
    "    in_channels=3,\n",
    "    classes=N_CLASSES,\n",
    ").to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=learning_rate, weight_decay=1e-6)\n",
    "\n",
    "train_transform = A.Compose([\n",
    "    A.Resize(image_size, image_size),\n",
    "    A.Normalize(\n",
    "        mean=(0.485, 0.456, 0.406),\n",
    "        std=(0.229, 0.224, 0.225), max_pixel_value=255.0, p=1.0\n",
    "    ),\n",
    "    A.HorizontalFlip(),\n",
    "    A.VerticalFlip(),\n",
    "    A.RandomRotate90(),\n",
    "    A.OneOf([\n",
    "        A.MotionBlur(p=1.0),\n",
    "        A.OpticalDistortion(p=1.0)\n",
    "    ], p=2/3),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "val_transform = A.Compose([\n",
    "    A.Resize(image_size, image_size),\n",
    "    A.Normalize(\n",
    "        mean=(0.485, 0.456, 0.406),\n",
    "        std=(0.229, 0.224, 0.225), max_pixel_value=255.0, p=1.0\n",
    "    ),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "test_transform = A.Compose([\n",
    "    A.Resize(image_size, image_size),\n",
    "    A.Normalize(\n",
    "        mean=(0.485, 0.456, 0.406),\n",
    "        std=(0.229, 0.224, 0.225), max_pixel_value=255.0, p=1.0\n",
    "    ),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "dataset_path = '../input/data'\n",
    "anns_file_path = dataset_path + '/train.json'\n",
    "train_path = dataset_path + '/train.json'\n",
    "val_path = dataset_path + '/val.json'\n",
    "test_path = dataset_path + '/test.json'\n",
    "saved_path = './saved'\n",
    "submission_path = './submission'\n",
    "category_names = ['Background','UNKNOWN','General trash','Paper','Paper pack','Metal','Glass','Plastic','Styrofoam','Plastic bag','Battery','Clothing']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=3.33s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.85s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=1.08s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "train_loader, val_loader, test_loader = get_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Start training\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/8 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "04799080eb3a45cbb5ac4886a7383dcb"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e610ab50844a4c878a3b89cc9e39b64a"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch[1/8], Step[25/327], Loss: 1.7211\n",
      "Epoch[1/8], Step[50/327], Loss: 1.3508\n",
      "Epoch[1/8], Step[75/327], Loss: 1.0605\n",
      "Epoch[1/8], Step[100/327], Loss: 1.1311\n",
      "Epoch[1/8], Step[125/327], Loss: 1.0520\n",
      "Epoch[1/8], Step[150/327], Loss: 0.5973\n",
      "Epoch[1/8], Step[175/327], Loss: 1.0081\n",
      "Epoch[1/8], Step[200/327], Loss: 0.6834\n",
      "Epoch[1/8], Step[225/327], Loss: 0.6153\n",
      "Epoch[1/8], Step[250/327], Loss: 0.5215\n",
      "Epoch[1/8], Step[275/327], Loss: 0.6252\n",
      "Epoch[1/8], Step[300/327], Loss: 0.4137\n",
      "Epoch[1/8], Step[325/327], Loss: 0.5960\n",
      "Start validation #1\n",
      "Validation #1 mIoU: 0.3119\n",
      "Best performance at epoch: 1\n",
      "Save model in ./saved\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b24e8e16f6cc49ada2b626aabb4fe971"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch[2/8], Step[25/327], Loss: 0.3097\n",
      "Epoch[2/8], Step[50/327], Loss: 0.4387\n",
      "Epoch[2/8], Step[75/327], Loss: 0.4734\n",
      "Epoch[2/8], Step[100/327], Loss: 0.5127\n",
      "Epoch[2/8], Step[125/327], Loss: 0.5353\n",
      "Epoch[2/8], Step[150/327], Loss: 0.7251\n",
      "Epoch[2/8], Step[175/327], Loss: 0.4467\n",
      "Epoch[2/8], Step[200/327], Loss: 0.5023\n",
      "Epoch[2/8], Step[225/327], Loss: 0.4381\n",
      "Epoch[2/8], Step[250/327], Loss: 0.4261\n",
      "Epoch[2/8], Step[275/327], Loss: 0.4292\n",
      "Epoch[2/8], Step[300/327], Loss: 0.3893\n",
      "Epoch[2/8], Step[325/327], Loss: 0.5046\n",
      "Start validation #2\n",
      "Validation #2 mIoU: 0.3851\n",
      "Best performance at epoch: 2\n",
      "Save model in ./saved\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "37267c1d5daf4ff29000b03d25b7493d"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch[3/8], Step[25/327], Loss: 0.4275\n",
      "Epoch[3/8], Step[50/327], Loss: 0.3513\n",
      "Epoch[3/8], Step[75/327], Loss: 0.2965\n",
      "Epoch[3/8], Step[100/327], Loss: 0.3348\n",
      "Epoch[3/8], Step[125/327], Loss: 0.2890\n",
      "Epoch[3/8], Step[150/327], Loss: 0.2922\n",
      "Epoch[3/8], Step[175/327], Loss: 0.6304\n",
      "Epoch[3/8], Step[200/327], Loss: 0.6884\n",
      "Epoch[3/8], Step[225/327], Loss: 0.2351\n",
      "Epoch[3/8], Step[250/327], Loss: 0.6968\n",
      "Epoch[3/8], Step[275/327], Loss: 0.3996\n",
      "Epoch[3/8], Step[300/327], Loss: 0.3882\n",
      "Epoch[3/8], Step[325/327], Loss: 0.3809\n",
      "Start validation #3\n",
      "Validation #3 mIoU: 0.4065\n",
      "Best performance at epoch: 3\n",
      "Save model in ./saved\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c4e08efcdeae48c0b4a418960ac55c80"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch[4/8], Step[25/327], Loss: 0.2248\n",
      "Epoch[4/8], Step[50/327], Loss: 0.2478\n",
      "Epoch[4/8], Step[75/327], Loss: 0.2160\n",
      "Epoch[4/8], Step[100/327], Loss: 0.3609\n",
      "Epoch[4/8], Step[125/327], Loss: 0.4634\n",
      "Epoch[4/8], Step[150/327], Loss: 0.3459\n",
      "Epoch[4/8], Step[175/327], Loss: 0.4653\n",
      "Epoch[4/8], Step[200/327], Loss: 0.3155\n",
      "Epoch[4/8], Step[225/327], Loss: 0.2935\n",
      "Epoch[4/8], Step[250/327], Loss: 0.3258\n",
      "Epoch[4/8], Step[275/327], Loss: 0.2770\n",
      "Epoch[4/8], Step[300/327], Loss: 0.3948\n",
      "Epoch[4/8], Step[325/327], Loss: 0.2751\n",
      "Start validation #4\n",
      "Validation #4 mIoU: 0.4160\n",
      "Best performance at epoch: 4\n",
      "Save model in ./saved\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4afe2aefcc094240a0f28e4675d88923"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch[5/8], Step[25/327], Loss: 0.2844\n",
      "Epoch[5/8], Step[50/327], Loss: 0.3364\n",
      "Epoch[5/8], Step[75/327], Loss: 0.4119\n",
      "Epoch[5/8], Step[100/327], Loss: 0.3751\n",
      "Epoch[5/8], Step[125/327], Loss: 0.2824\n",
      "Epoch[5/8], Step[150/327], Loss: 0.2707\n",
      "Epoch[5/8], Step[175/327], Loss: 0.3724\n",
      "Epoch[5/8], Step[200/327], Loss: 0.2773\n",
      "Epoch[5/8], Step[225/327], Loss: 0.3381\n",
      "Epoch[5/8], Step[250/327], Loss: 0.3235\n",
      "Epoch[5/8], Step[275/327], Loss: 0.2598\n",
      "Epoch[5/8], Step[300/327], Loss: 0.2661\n",
      "Epoch[5/8], Step[325/327], Loss: 0.3596\n",
      "Start validation #5\n",
      "Validation #5 mIoU: 0.4467\n",
      "Best performance at epoch: 5\n",
      "Save model in ./saved\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "89bae0cc1bb645acb646971659c5b088"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch[6/8], Step[25/327], Loss: 0.6224\n",
      "Epoch[6/8], Step[50/327], Loss: 0.3787\n",
      "Epoch[6/8], Step[75/327], Loss: 0.1795\n",
      "Epoch[6/8], Step[100/327], Loss: 0.2821\n",
      "Epoch[6/8], Step[125/327], Loss: 0.3501\n",
      "Epoch[6/8], Step[150/327], Loss: 0.4985\n",
      "Epoch[6/8], Step[175/327], Loss: 0.2149\n",
      "Epoch[6/8], Step[200/327], Loss: 0.2919\n",
      "Epoch[6/8], Step[225/327], Loss: 0.1563\n",
      "Epoch[6/8], Step[250/327], Loss: 0.4361\n",
      "Epoch[6/8], Step[275/327], Loss: 0.3546\n",
      "Epoch[6/8], Step[300/327], Loss: 0.4541\n",
      "Epoch[6/8], Step[325/327], Loss: 0.3356\n",
      "Start validation #6\n",
      "Validation #6 mIoU: 0.4532\n",
      "Best performance at epoch: 6\n",
      "Save model in ./saved\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2ec8f023d69d417486a9645d2b182e12"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch[7/8], Step[25/327], Loss: 0.6485\n",
      "Epoch[7/8], Step[50/327], Loss: 0.1317\n",
      "Epoch[7/8], Step[75/327], Loss: 0.3423\n",
      "Epoch[7/8], Step[100/327], Loss: 0.2513\n",
      "Epoch[7/8], Step[125/327], Loss: 0.3854\n",
      "Epoch[7/8], Step[150/327], Loss: 0.5616\n",
      "Epoch[7/8], Step[175/327], Loss: 0.4852\n",
      "Epoch[7/8], Step[200/327], Loss: 0.3009\n",
      "Epoch[7/8], Step[225/327], Loss: 0.2075\n",
      "Epoch[7/8], Step[250/327], Loss: 0.5768\n",
      "Epoch[7/8], Step[275/327], Loss: 0.3902\n",
      "Epoch[7/8], Step[300/327], Loss: 0.3493\n",
      "Epoch[7/8], Step[325/327], Loss: 0.1613\n",
      "Start validation #7\n",
      "Validation #7 mIoU: 0.4388\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "274624dbc9cd4dc0a4bd9d80e0cb0bd6"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch[8/8], Step[25/327], Loss: 0.2564\n",
      "Epoch[8/8], Step[50/327], Loss: 0.4046\n",
      "Epoch[8/8], Step[75/327], Loss: 0.2359\n",
      "Epoch[8/8], Step[100/327], Loss: 0.2232\n",
      "Epoch[8/8], Step[125/327], Loss: 0.1749\n",
      "Epoch[8/8], Step[150/327], Loss: 0.4186\n",
      "Epoch[8/8], Step[175/327], Loss: 0.2502\n",
      "Epoch[8/8], Step[200/327], Loss: 0.1715\n",
      "Epoch[8/8], Step[225/327], Loss: 0.2221\n",
      "Epoch[8/8], Step[250/327], Loss: 0.2041\n",
      "Epoch[8/8], Step[275/327], Loss: 0.3181\n",
      "Epoch[8/8], Step[300/327], Loss: 0.3642\n",
      "Epoch[8/8], Step[325/327], Loss: 0.1797\n",
      "Start validation #8\n",
      "Validation #8 mIoU: 0.4322\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[0.3118551614338785,\n",
       " 0.385111440425622,\n",
       " 0.4064766708411446,\n",
       " 0.41596951649921626,\n",
       " 0.4467292604135856,\n",
       " 0.4532302930833944,\n",
       " 0.438831117628027,\n",
       " 0.43224035722611304]"
      ]
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "source": [
    "train(num_epochs, model, train_loader, val_loader, criterion, optimizer, saved_path, val_every, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(model_name)\n",
    "make_submission(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_test(encoder_list, decoder_list):\n",
    "    model_test_result = {}\n",
    "    for encoder in encoder_list:\n",
    "        for  decoder in decoder_list:\n",
    "            if decoder == 'DeepLabV3Plus':\n",
    "                model = smp.DeepLabV3Plus(\n",
    "                    encoder_name=encoder,\n",
    "                    encoder_weights=\"imagenet\",\n",
    "                    in_channels=3,\n",
    "                    classes=N_CLASSES,\n",
    "                )\n",
    "            else:\n",
    "                raise 'error'\n",
    "            model = model.to(device)\n",
    "            result = train(num_epochs, model, train_loader, val_loader, criterion, optimizer, saved_path, val_every, device, print_log=False)\n",
    "            model_test_result[encoder + ' ' + decoder] = result\n",
    "    return model_test_result"
   ]
  }
 ]
}