    def get_folded_dataloader(kfold=5):
        train_dataset = CustomDataset(data_dir=train_all_path, mode='train',transform=train_transform)

        train_all_size = len(train_dataset)
        size_list = [train_all_size // kfold] * kfold
        size_list = [train_all_size // kfold] * kfold
        for i in range(train_all_size % kfold):
            size_list[i] += 1
        train_dataset_list = torch.utils.data.random_split(train_dataset, size_list)

        def collate_fn(batch):
            return tuple(zip(*batch))

        for k in range(kfold):
            train_loader = torch.utils.data.ConcatDataset(train_dataset_list[:k] + train_dataset_list[k + 1:])
            val_loader = train_dataset_list[k]

            train_loader = torch.utils.data.DataLoader(dataset=train_loader, 
                                                        batch_size=batch_size,
                                                        shuffle=True,
                                                        num_workers=4,
                                                        collate_fn=collate_fn,
                                                        drop_last=True,
                                                        worker_init_fn=seed_worker)

            val_loader = torch.utils.data.DataLoader(dataset=val_loader, 
                                                        batch_size=batch_size,
                                                        shuffle=True,
                                                        num_workers=4,
                                                        collate_fn=collate_fn,
                                                        drop_last=True,
                                                        worker_init_fn=seed_worker)

            yield train_loader, val_loader

제너레이터로 만들어서 간편하게 사용할 수 있다.

    kfold = 5
    for k, (train_loader, val_loader) in enumerate(get_folded_dataloader(kfold)):
        model = smp.DeepLabV3Plus(
            encoder_name="timm-regnetx_064",
            encoder_weights="imagenet",
            in_channels=3,
            classes=N_CLASSES,
        ).to(device)
        criterion = nn.CrossEntropyLoss()
        optimizer = torch.optim.Adam(params=model.parameters(), lr=learning_rate, weight_decay=1e-6)
        
        #!!kfold할 때마다 모델 저장시 파일 이름은 매번 바꿔줘야 덮어씌워지지 않아요!!

        train(num_epochs, model, train_loader, val_loader, criterion, optimizer, saved_path)

이런 식으로 for문에 넣어서 사용할 수 있다.
