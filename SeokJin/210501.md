210501_1_my_baseline_optim.ipynb 파일으로 모델만 바꾸어가며 실험하였다. 모델은 모두 segmentation_model에 있는 것이다.

각 실험마다 인코더, 디코더, 인코더 가중치, 8에폭을 돌면서 나온 mIoU가 기록되어있다.

<details>
<summary>공통 하이퍼 파라미터</summary>
  
    N_CLASSES = 12
    batch_size = 8
    num_epochs = 8
    image_size = 256
    learning_rate = 0.0001
    model_name = 'model_test'
    random_seed = 21
    val_every = 1

    criterion = nn.CrossEntropyLoss()
    optimizer = torch.optim.Adam(params=model.parameters(), lr=learning_rate, weight_decay=1e-6)

    train_transform = A.Compose([
        A.Resize(image_size, image_size),
        A.Normalize(
            mean=(0.485, 0.456, 0.406),
            std=(0.229, 0.224, 0.225), max_pixel_value=255.0, p=1.0
        ),
        A.HorizontalFlip(),
        A.VerticalFlip(),
        A.RandomRotate90(),
        A.OneOf([
            A.MotionBlur(p=1.0),
            A.OpticalDistortion(p=1.0)
        ], p=2/3),
        ToTensorV2()
    ])

    val_transform = A.Compose([
        A.Resize(image_size, image_size),
        A.Normalize(
            mean=(0.485, 0.456, 0.406),
            std=(0.229, 0.224, 0.225), max_pixel_value=255.0, p=1.0
        ),
        ToTensorV2()
    ])

    test_transform = A.Compose([
        A.Resize(image_size, image_size),
        A.Normalize(
            mean=(0.485, 0.456, 0.406),
            std=(0.229, 0.224, 0.225), max_pixel_value=255.0, p=1.0
        ),
        ToTensorV2()
    ])

    dataset_path = '../input/data'
    anns_file_path = dataset_path + '/train.json'
    train_path = dataset_path + '/train.json'
    val_path = dataset_path + '/val.json'
    test_path = dataset_path + '/test.json'
    saved_path = './saved'
    submission_path = './submission'
    category_names = ['Background','UNKNOWN','General trash','Paper','Paper pack','Metal','Glass','Plastic','Styrofoam','Plastic bag','Battery','Clothing']

</details>

시드 고정 문제

아무리 시드를 고정하더라도 deterministic하게 바꿀 수 없는 함수가 존재해서 매 실행마다 완전히 똑같은 성능을 기대할 수는 없다. 평균적으로 0.002정도 차이가 나는 것 같다.
서로 다른 두 모델을 비교할 땐 0.004정도의 차이가 무시될 수 있다 보면 될 것 같다. 넉넉잡아 0.01정도의 차이는 무시하자. 그 이상 차이나면 모델간에 성능차이가 분명히 존재하다고 할 수 있겠다.

<details>
<summary>예시</summary>
  
    첫 번째 시도
    Epoch[1/8], Step[25/327], Loss: 1.8001
    Epoch[1/8], Step[50/327], Loss: 1.2874
    Epoch[1/8], Step[75/327], Loss: 1.0651
    Epoch[1/8], Step[100/327], Loss: 1.0220
    Epoch[1/8], Step[125/327], Loss: 0.8040
    Epoch[1/8], Step[150/327], Loss: 0.8964
    Epoch[1/8], Step[175/327], Loss: 0.6979
    Epoch[1/8], Step[200/327], Loss: 0.6790
    Epoch[1/8], Step[225/327], Loss: 0.7668
    Epoch[1/8], Step[250/327], Loss: 0.4986
    Epoch[1/8], Step[275/327], Loss: 0.5808
    Epoch[1/8], Step[300/327], Loss: 0.5706
    Epoch[1/8], Step[325/327], Loss: 0.4420
    Start validation #1
    Validation #1 mIoU: 0.3444
    두 번째 시도
    Epoch[1/8], Step[25/327], Loss: 1.7986
    Epoch[1/8], Step[50/327], Loss: 1.2883
    Epoch[1/8], Step[75/327], Loss: 1.0657
    Epoch[1/8], Step[100/327], Loss: 1.0234
    Epoch[1/8], Step[125/327], Loss: 0.7964
    Epoch[1/8], Step[150/327], Loss: 0.9032
    Epoch[1/8], Step[175/327], Loss: 0.7021
    Epoch[1/8], Step[200/327], Loss: 0.6928
    Epoch[1/8], Step[225/327], Loss: 0.7593
    Epoch[1/8], Step[250/327], Loss: 0.4945
    Epoch[1/8], Step[275/327], Loss: 0.5916
    Epoch[1/8], Step[300/327], Loss: 0.5538
    Epoch[1/8], Step[325/327], Loss: 0.4414
    Start validation #1
    Validation #1 mIoU: 0.3462
    
</details>

시드 고정은 언제 정확히 해야 하는지 몰라서 시드 고정 -> 하이퍼 파라미터 설정/데이터 로더 생성 등등 -> 시드 고정 -> 모델 생성 -> 학습 -> 시드 고정 -> 모델 생성 -> 학습 -> 시드 고정 -> 모델 생성 -> 학습 -> 반복 하였다.

segmentation_model 중 파라미터 수가 50M 이하인 것만 일단 모두 수행해보았다.

## 1.
Encoder: resnext50_32x4d

Decoder: DeepLabV3Plus

Encoder_weight: imagenet

    Spent Time
    

    mIoU
    [0.34622305389272984,
     0.38685328411838443,
     0.43470663338313537,
     0.4591151894932782,
     0.43820937125616555,
     0.46874905557651475,
     0.4332026292586379,
     0.4722891526762096]


## 2.


<details>
<summary>접기/펼치기</summary>



</details>
