## 1. mIoU : 0.5737

    여러 모델을 테스트 한 것 중 0.49가 나왔던 것을 20에폭으로 늘려 학습하고 제출하였더니 성적이 좋았다.
    batch_size = 16
    num_epochs = 20
    image_size = 256
    learning_rate = 0.0001
    random_seed = 21

    decoder : DeepLabV3Plus
    encoder : timm-regnetx_064
    encoder_weights : imagenet,
    criterion : CrossEntropyLoss
    optimizer : Adam - lr = learning_rate, weight_decay = 1e-6

    train_transform = A.Compose([
        A.Resize(image_size, image_size),
        A.Normalize(
            mean=(0.485, 0.456, 0.406),
            std=(0.229, 0.224, 0.225), max_pixel_value=255.0, p=1.0
        ),
        A.HorizontalFlip(),
        A.VerticalFlip(),
        A.RandomRotate90(),
        A.OneOf([
            A.MotionBlur(p=1.0),
            A.OpticalDistortion(p=1.0)
        ], p=2/3),
        ToTensorV2()
    ])

    val_transform = A.Compose([
        A.Resize(image_size, image_size),
        A.Normalize(
            mean=(0.485, 0.456, 0.406),
            std=(0.229, 0.224, 0.225), max_pixel_value=255.0, p=1.0
        ),
        ToTensorV2()
    ])

    test_transform = A.Compose([
        A.Resize(image_size, image_size),
        A.Normalize(
            mean=(0.485, 0.456, 0.406),
            std=(0.229, 0.224, 0.225), max_pixel_value=255.0, p=1.0
        ),
        ToTensorV2()
    ])

## 2. mIoU : 0.5785
    
    1번 모델을 20에폭 더, 총 40에폭을 돌렸더니 성능이 약간 올랐다.
