{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python377jvsc74a57bd098b0a9b7b4eaaa670588a142fd0a9b87eaafe866f1db4228be72b4211d12040f",
   "display_name": "Python 3.7.7 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "/opt/ml/code\n"
     ]
    }
   ],
   "source": [
    "cd /opt/ml/code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "pytorch version: 1.7.1\nGPU 사용 가능 여부: True\nTesla P40\n1\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import json\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import segmentation_models_pytorch as smp\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from utils_modified import label_accuracy_score, add_hist\n",
    "\n",
    "import cv2\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pycocotools.coco import COCO\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import timeit\n",
    "\n",
    "print(f'pytorch version: {torch.__version__}')\n",
    "print(f'GPU 사용 가능 여부: {torch.cuda.is_available()}')\n",
    "print(torch.cuda.get_device_name(0))\n",
    "print(torch.cuda.device_count())\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=21):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    #torch.backends.cudnn.deterministic = True\n",
    "    #torch.backends.cudnn.benchmark = False\n",
    "    #torch.set_deterministic(True)\n",
    "\n",
    "def seed_worker(worker_id):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EDA():\n",
    "    with open(anns_file_path, 'r') as f:\n",
    "        dataset = json.loads(f.read())\n",
    "\n",
    "    categories = dataset['categories']\n",
    "    anns = dataset['annotations']\n",
    "    imgs = dataset['images']\n",
    "    nr_cats = len(categories)\n",
    "    nr_annotations = len(anns)\n",
    "    nr_images = len(imgs)\n",
    "\n",
    "    cat_names = []\n",
    "    super_cat_names = []\n",
    "    super_cat_ids = {}\n",
    "    super_cat_last_name = ''\n",
    "    nr_super_cats = 0\n",
    "    for cat_it in categories:\n",
    "        cat_names.append(cat_it['name'])\n",
    "        super_cat_name = cat_it['supercategory']\n",
    "        if super_cat_name != super_cat_last_name:\n",
    "            super_cat_names.append(super_cat_name)\n",
    "            super_cat_ids[super_cat_name] = nr_super_cats\n",
    "            nr_super_cats += 1\n",
    "    print('Number of super categories:', nr_super_cats)\n",
    "    print('Number of categories:', nr_cats)\n",
    "    print('Number of annotations:', nr_annotations)\n",
    "    print('Number of images:', nr_images)\n",
    "\n",
    "    cat_histogram = np.zeros(nr_cats, dtype=int)\n",
    "    for ann_it in anns:\n",
    "        cat_histogram[ann_it['category_id']] += 1\n",
    "\n",
    "    f, ax = plt.subplots(figsize=(5, 5))\n",
    "    df = pd.DataFrame({'Categories': cat_names, 'Number of annotations':cat_histogram})\n",
    "\n",
    "    plt.title('category distribution of train set')\n",
    "    sns.barplot(x='Number of annotations', y='Categories', data=df.sort_values('Number of annotations', ascending=False), label='Total', color='b')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data_dir, mode, transform):\n",
    "        super().__init__()\n",
    "        self.mode = mode\n",
    "        self.transform = transform\n",
    "        self.coco = COCO(data_dir)\n",
    "    \n",
    "    def __getitem__(self, index:int):\n",
    "        image_infos = self.coco.loadImgs(index)[0]\n",
    "\n",
    "        images = cv2.imread(os.path.join(dataset_path, image_infos['file_name']))\n",
    "        images = cv2.cvtColor(images, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "        #images /= 255.0\n",
    "\n",
    "        if self.mode in ('train', 'val'):\n",
    "            ann_ids = self.coco.getAnnIds(imgIds=image_infos['id'])\n",
    "            anns = self.coco.loadAnns(ann_ids)\n",
    "\n",
    "            masks = np.zeros((image_infos['height'], image_infos['width']))\n",
    "            for ann in anns:\n",
    "                masks = np.maximum(self.coco.annToMask(ann) * (ann['category_id'] + 1), masks)\n",
    "            \n",
    "            images, masks = self.transform(image=images, mask=masks).values()\n",
    "            return images, masks, image_infos\n",
    "        \n",
    "        if self.mode == 'test':\n",
    "            images = self.transform(image=images)['image']\n",
    "            return images, image_infos\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.coco.getImgIds())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloader():\n",
    "    train_dataset = CustomDataset(data_dir=train_path, mode='train',transform=train_transform)\n",
    "    val_dataset = CustomDataset(data_dir=val_path, mode='val', transform=val_transform)\n",
    "    test_dataset = CustomDataset(data_dir=test_path, mode='test', transform=test_transform)\n",
    "\n",
    "    def collate_fn(batch):\n",
    "        return tuple(zip(*batch))\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                            batch_size=batch_size,\n",
    "                                            shuffle=True,\n",
    "                                            num_workers=4,\n",
    "                                            collate_fn=collate_fn,\n",
    "                                            drop_last=True,\n",
    "                                            worker_init_fn=seed_worker)\n",
    "\n",
    "    val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                            batch_size=batch_size,\n",
    "                                            shuffle=False,\n",
    "                                            num_workers=4,\n",
    "                                            collate_fn=collate_fn,\n",
    "                                            worker_init_fn=seed_worker)\n",
    "\n",
    "    test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                            batch_size=batch_size,\n",
    "                                            num_workers=4,\n",
    "                                            collate_fn=collate_fn,\n",
    "                                            worker_init_fn=seed_worker)\n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_folded_dataloader(kfold=5):\n",
    "    train_dataset = CustomDataset(data_dir=train_all_path, mode='train',transform=train_transform)\n",
    "\n",
    "    train_all_size = len(train_dataset)\n",
    "    size_list = [train_all_size // kfold] * kfold\n",
    "    size_list = [train_all_size // kfold] * kfold\n",
    "    for i in range(train_all_size % kfold):\n",
    "        size_list[i] += 1\n",
    "    train_dataset_list = torch.utils.data.random_split(train_dataset, size_list)\n",
    "\n",
    "    def collate_fn(batch):\n",
    "        return tuple(zip(*batch))\n",
    "\n",
    "    for k in range(kfold):\n",
    "        train_loader = torch.utils.data.ConcatDataset(train_dataset_list[:k] + train_dataset_list[k + 1:])\n",
    "        val_loader = train_dataset_list[k]\n",
    "\n",
    "        train_loader = torch.utils.data.DataLoader(dataset=train_loader, \n",
    "                                                    batch_size=batch_size,\n",
    "                                                    shuffle=True,\n",
    "                                                    num_workers=4,\n",
    "                                                    collate_fn=collate_fn,\n",
    "                                                    drop_last=True,\n",
    "                                                    worker_init_fn=seed_worker)\n",
    "        \n",
    "        val_loader = torch.utils.data.DataLoader(dataset=val_loader, \n",
    "                                                    batch_size=batch_size,\n",
    "                                                    shuffle=True,\n",
    "                                                    num_workers=4,\n",
    "                                                    collate_fn=collate_fn,\n",
    "                                                    drop_last=True,\n",
    "                                                    worker_init_fn=seed_worker)\n",
    "\n",
    "        yield train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_dataloader(dataloader):\n",
    "    data = iter(dataloader).next()\n",
    "    if len(data) == 3:\n",
    "        imgs, masks, image_infos = data\n",
    "        img = imgs[0]\n",
    "        mask = masks[0]\n",
    "        image_info = image_infos[0]\n",
    "\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(12, 12))\n",
    "        print('image shape:', list(img.shape))\n",
    "        print('mask shape:', list(mask.shape))\n",
    "        print('Unique values, category of transformed mask:\\n', {int(i):category_names[int(i)] for i in list(np.unique(mask))})\n",
    "\n",
    "        axes[0].imshow(img.permute([1, 2, 0]))\n",
    "        axes[0].grid(False)\n",
    "        axes[0].set_title('imput image:' + str(image_info['file_name']), fontsize=15)\n",
    "\n",
    "        axes[1].imshow(mask)\n",
    "        axes[1].grid(False)\n",
    "        axes[1].set_title('masks :' + str(image_info['file_name']), fontsize=15)\n",
    "\n",
    "        plt.show()\n",
    "    elif len(data) == 2:\n",
    "        imgs, image_infos = iter(dataloader).next()\n",
    "        img = imgs[0]\n",
    "        image_info = image_infos[0]\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(6, 6))\n",
    "        print('image shape:', list(img.shape))\n",
    "\n",
    "        ax.imshow(img.permute([1, 2, 0]))\n",
    "        ax.grid(False)\n",
    "        ax.set_title('imput image:' + str(image_info['file_name']), fontsize=15)\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(num_epochs, model, data_loader, val_loader, criterion, optimizer, saved_dir, val_every, device, print_log=True):\n",
    "    if print_log:\n",
    "        print('Start training')\n",
    "    best_mIoU = 0\n",
    "    hist_mIoU = []\n",
    "    epoch_begin = 0\n",
    "\n",
    "    name = str(type(model.encoder)).split('.')[-1]\n",
    "    name += ' ' + str(type(model.decoder)).split('.')[-1]\n",
    "\n",
    "    for epoch in tqdm(range(0, num_epochs), desc=name):\n",
    "        model.train()\n",
    "        for step, (images, masks, _) in tqdm(enumerate(data_loader), desc='Training', leave=False, total=len(data_loader)):\n",
    "            images = torch.stack(images).to(device)\n",
    "            masks = torch.stack(masks).long().to(device)\n",
    "            outputs = model(images)\n",
    "            \n",
    "            loss = criterion(outputs, masks)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if print_log:\n",
    "                if (step + 1) % 25 == 0:\n",
    "                    print(f'Epoch[{epoch + 1}/{num_epochs}], Step[{step + 1}/{len(data_loader)}], Loss: {loss.item():.4f}')\n",
    "                    \n",
    "        if (epoch + 1) % val_every == 0:\n",
    "            mIoU = validation(epoch + 1, model, val_loader, criterion, device, print_log)\n",
    "            hist_mIoU.append(mIoU)\n",
    "            if mIoU > best_mIoU:\n",
    "                if print_log:\n",
    "                    print(f'Best performance at epoch: {epoch + 1}')\n",
    "                    print('Save model in', saved_dir)\n",
    "                best_mIoU = mIoU\n",
    "                save_model(model, saved_dir)\n",
    "\n",
    "    return hist_mIoU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(epoch, model, data_loader, criterion, device, print_log=True):\n",
    "    if print_log:\n",
    "        print(f'Start validation #{epoch}')\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        hist = np.zeros((N_CLASSES, N_CLASSES))\n",
    "        for step, (images, masks, _) in tqdm(enumerate(data_loader), desc='Validation', leave=False, total=len(data_loader)):\n",
    "            images = torch.stack(images).to(device)\n",
    "            masks = torch.stack(masks).long().to(device)\n",
    "            outputs = model(images)\n",
    "\n",
    "            loss = criterion(outputs, masks)\n",
    "\n",
    "            outputs = torch.argmax(outputs.squeeze(), dim=1)\n",
    "            outputs = outputs.detach().cpu().numpy()\n",
    "            masks = masks.detach().cpu().numpy()\n",
    "            hist = add_hist(hist, masks, outputs, n_class=N_CLASSES)\n",
    "        \n",
    "        acc, acc_cls, mIoU, fwavacc = label_accuracy_score(hist)\n",
    "        if print_log:\n",
    "            print(f'Validation #{epoch} mIoU: {mIoU:.4f}')\n",
    "    return mIoU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, saved_dir):\n",
    "    file_name = f'{model_name}.pt'\n",
    "    os.makedirs(saved_dir, exist_ok=True)\n",
    "    check_point = {'net':model.state_dict()}\n",
    "    output_path = os.path.join(saved_dir, file_name)\n",
    "    torch.save(model.state_dict(), output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_name, model):\n",
    "    model_path = saved_path + f'/{model_name}.pt'\n",
    "    checkpoint = torch.load(model_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_eval_model(model, dataloader, idx=0):\n",
    "    for imgs, image_infos in dataloader:\n",
    "        model.eval()\n",
    "        outs = model(torch.stack(imgs).to(device))\n",
    "        outs = torch.argmax(outs.squeeze(), dim=1).detach().cpu().numpy()\n",
    "        break\n",
    "\n",
    "    imgs = imgs[idx]\n",
    "    image_infos = image_infos[idx]\n",
    "    outs = outs[idx]\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 16))\n",
    "    print('Shape of Original Image:', list(imgs.shape))\n",
    "    print('Shape of Predicted:', list(outs.shape))\n",
    "    print('Unique values, category of transformed mask\\n', {int(i):category_names[int(i)] for i in list(np.unique(outs))})\n",
    "\n",
    "    axes[0].imshow(imgs.permute([1,2,0]))\n",
    "    axes[0].grid(False)\n",
    "    axes[0].set_title('Original image:' + str(image_infos['file_name']), fontsize=15)\n",
    "\n",
    "    axes[1].imshow(outs)\n",
    "    axes[1].grid(False)\n",
    "    axes[1].set_title('Predicted:' + str(image_infos['file_name']), fontsize=15)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model_list, data_loader, device):\n",
    "    size = 256\n",
    "    transform = A.Compose([A.Resize(256, 256)])\n",
    "    print('Start prediction')\n",
    "    for model in model_list:\n",
    "        model.eval()\n",
    "\n",
    "    file_name_list = []\n",
    "    preds_array = np.empty((0, size*size), dtype=np.long)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for step, (imgs, image_infos) in tqdm(enumerate(data_loader), total=len(data_loader)):\n",
    "            outs = None\n",
    "            for model in model_list:\n",
    "                if outs is None:\n",
    "                    outs = model(torch.stack(imgs).to(device))\n",
    "                else:\n",
    "                    outs += model(torch.stack(imgs).to(device))\n",
    "            outs = torch.argmax(outs, dim=1).detach().cpu().numpy()\n",
    "\n",
    "            masks = []\n",
    "            for img, mask in zip(np.stack(imgs), outs):\n",
    "                img, mask = transform(image=img, mask=mask).values()\n",
    "                masks.append(mask)\n",
    "            outs = np.array(masks)\n",
    "            outs = outs.reshape([outs.shape[0], size * size]).astype(int)\n",
    "            preds_array = np.vstack((preds_array, outs))\n",
    "\n",
    "            file_name_list.append([i['file_name'] for i in image_infos])\n",
    "    print('End prediction.')\n",
    "    file_names = [y for x in file_name_list for y in x]\n",
    "    return file_names, preds_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_submission(model_list, dataloader):\n",
    "    submission = pd.read_csv('./submission/sample_submission.csv', index_col=None)\n",
    "    file_names, preds = test(model_list, dataloader, device)\n",
    "\n",
    "    for file_name, string in zip(file_names, preds):\n",
    "        submission = submission.append({'image_id':file_name, 'PredictionString':' '.join(str(e) for e in string.tolist())}, ignore_index=True)\n",
    "\n",
    "    submission.to_csv(submission_path + f'/{model_name}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_test(encoder_list, decoder_list):\n",
    "    model_test_result = []\n",
    "    for encoder in encoder_list:\n",
    "        for  decoder in decoder_list:\n",
    "            set_seed(random_seed)\n",
    "            try:\n",
    "                if decoder == 'DeepLabV3Plus':\n",
    "                    model = smp.DeepLabV3Plus(\n",
    "                        encoder_name=encoder,\n",
    "                        encoder_weights=\"imagenet\",\n",
    "                        in_channels=3,\n",
    "                        classes=N_CLASSES,\n",
    "                    )\n",
    "                else:\n",
    "                    raise 'decoder does not exist'\n",
    "                criterion = nn.CrossEntropyLoss()\n",
    "                optimizer = torch.optim.Adam(params=model.parameters(), lr=learning_rate, weight_decay=1e-6)\n",
    "                model = model.to(device)\n",
    "                start_time = timeit.default_timer()\n",
    "                mIoU = train(num_epochs, model, train_loader, val_loader, criterion, optimizer, saved_path, val_every, device, print_log=False)\n",
    "                end_time = timeit.default_timer()\n",
    "                time = end_time - start_time\n",
    "                stat = {'encoder' : encoder, 'decoder' : decoder, 'mIoU' : mIoU, 'time' : time}\n",
    "            except Exception as e:\n",
    "                stat = {'encoder' : encoder, 'decoder' : decoder, 'error' : e}\n",
    "                pass\n",
    "            print(stat)\n",
    "            model_test_result.append(stat)\n",
    "    return model_test_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_CLASSES = 12\n",
    "batch_size = 16\n",
    "num_epochs = 10\n",
    "image_size = 256\n",
    "learning_rate = 0.0001\n",
    "model_name = 'model_test'\n",
    "random_seed = 21\n",
    "val_every = 1\n",
    "\n",
    "set_seed(random_seed)\n",
    "\n",
    "model = smp.DeepLabV3Plus(\n",
    "    encoder_name=\"timm-regnetx_064\",\n",
    "    encoder_weights=\"imagenet\",\n",
    "    in_channels=3,\n",
    "    classes=N_CLASSES,\n",
    ").to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=learning_rate, weight_decay=1e-6)\n",
    "\n",
    "train_transform = A.Compose([\n",
    "    A.Resize(image_size, image_size),\n",
    "    A.Normalize(\n",
    "        mean=(0.485, 0.456, 0.406),\n",
    "        std=(0.229, 0.224, 0.225), max_pixel_value=255.0, p=1.0\n",
    "    ),\n",
    "    A.HorizontalFlip(),\n",
    "    A.VerticalFlip(),\n",
    "    A.RandomRotate90(),\n",
    "    A.OneOf([\n",
    "        A.MotionBlur(p=1.0),\n",
    "        A.OpticalDistortion(p=1.0)\n",
    "    ], p=2/3),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "val_transform = A.Compose([\n",
    "    A.Resize(image_size, image_size),\n",
    "    A.Normalize(\n",
    "        mean=(0.485, 0.456, 0.406),\n",
    "        std=(0.229, 0.224, 0.225), max_pixel_value=255.0, p=1.0\n",
    "    ),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "test_transform = A.Compose([\n",
    "    A.Resize(image_size, image_size),\n",
    "    A.Normalize(\n",
    "        mean=(0.485, 0.456, 0.406),\n",
    "        std=(0.229, 0.224, 0.225), max_pixel_value=255.0, p=1.0\n",
    "    ),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "dataset_path = '../input/data'\n",
    "anns_file_path = dataset_path + '/train.json'\n",
    "train_path = dataset_path + '/train.json'\n",
    "train_all_path = dataset_path + '/train_all.json'\n",
    "val_path = dataset_path + '/val.json'\n",
    "test_path = dataset_path + '/test.json'\n",
    "saved_path = './saved'\n",
    "submission_path = './submission'\n",
    "category_names = ['Background','UNKNOWN','General trash','Paper','Paper pack','Metal','Glass','Plastic','Styrofoam','Plastic bag','Battery','Clothing']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=4.22s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "RegNetEncoder'> DeepLabV3PlusDecoder'>:   0%|          | 0/10 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "759400a39a4e452388ba176b84b15f2b"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Training:   0%|          | 0/163 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f6c624deeb0c4c37ad82bdba235cc77b"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-6b5f40d271f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaved_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_every\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-09c87352aa69>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(num_epochs, model, data_loader, val_loader, criterion, optimizer, saved_dir, val_every, device, print_log)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Training'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleave\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m             \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m             \u001b[0mmasks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "kfold = 5\n",
    "for k, (train_loader, val_loader) in enumerate(get_folded_dataloader(kfold)):\n",
    "    model_name = f'timm-regnetx_064_{k}-{kfold}fold'\n",
    "    model = smp.DeepLabV3Plus(\n",
    "        encoder_name=\"timm-regnetx_064\",\n",
    "        encoder_weights=\"imagenet\",\n",
    "        in_channels=3,\n",
    "        classes=N_CLASSES,\n",
    "    ).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(params=model.parameters(), lr=learning_rate, weight_decay=1e-6)\n",
    "\n",
    "    train(num_epochs, model, train_loader, val_loader, criterion, optimizer, saved_path, val_every, device, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=3.60s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.85s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.01s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "_, _, test_loader = get_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Start prediction\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/53 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7100a3cb7f0e4356a77d386991fba1e5"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "End prediction.\n"
     ]
    }
   ],
   "source": [
    "ensamble_path = [\n",
    "    '/opt/ml/code/saved/timm-regnetx_064_0-5fold.pt',\n",
    "    '/opt/ml/code/saved/timm-regnetx_064_1-5fold.pt',\n",
    "    '/opt/ml/code/saved/timm-regnetx_064_2-5fold.pt',\n",
    "    '/opt/ml/code/saved/timm-regnetx_064_3-5fold.pt',\n",
    "    '/opt/ml/code/saved/timm-regnetx_064_4-5fold.pt'\n",
    "]\n",
    "model_list = []\n",
    "for p in ensamble_path:\n",
    "    model_path = p\n",
    "    checkpoint = torch.load(model_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint)\n",
    "    model_list.append(model)\n",
    "make_submission(model_list, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Start prediction\nDeepLabV3Plus(\n  (encoder): RegNetEncoder(\n    (stem): ConvBnAct(\n      (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn): BatchNormAct2d(\n        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n        (act): ReLU(inplace=True)\n      )\n    )\n    (s1): RegStage(\n      (b1): Bottleneck(\n        (conv1): ConvBnAct(\n          (conv): Conv2d(32, 168, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNormAct2d(\n            168, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n            (act): ReLU(inplace=True)\n          )\n        )\n        (conv2): ConvBnAct(\n          (conv): Conv2d(168, 168, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=3, bias=False)\n          (bn): BatchNormAct2d(\n            168, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n            (act): ReLU(inplace=True)\n          )\n        )\n        (conv3): ConvBnAct(\n          (conv): Conv2d(168, 168, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNormAct2d(168, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (act3): ReLU(inplace=True)\n        (downsample): ConvBnAct(\n          (conv): Conv2d(32, 168, kernel_size=(1, 1), stride=(2, 2), bias=False)\n          (bn): BatchNormAct2d(168, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (b2): Bottleneck(\n        (conv1): ConvBnAct(\n          (conv): Conv2d(168, 168, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNormAct2d(\n            168, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n            (act): ReLU(inplace=True)\n          )\n        )\n        (conv2): ConvBnAct(\n          (conv): Conv2d(168, 168, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=3, bias=False)\n          (bn): BatchNormAct2d(\n            168, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n            (act): ReLU(inplace=True)\n          )\n        )\n        (conv3): ConvBnAct(\n          (conv): Conv2d(168, 168, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNormAct2d(168, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (act3): ReLU(inplace=True)\n      )\n    )\n    (s2): RegStage(\n      (b1): Bottleneck(\n        (conv1): ConvBnAct(\n          (conv): Conv2d(168, 392, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNormAct2d(\n            392, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n            (act): ReLU(inplace=True)\n          )\n        )\n        (conv2): ConvBnAct(\n          (conv): Conv2d(392, 392, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=7, bias=False)\n          (bn): BatchNormAct2d(\n            392, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n            (act): ReLU(inplace=True)\n          )\n        )\n        (conv3): ConvBnAct(\n          (conv): Conv2d(392, 392, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNormAct2d(392, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (act3): ReLU(inplace=True)\n        (downsample): ConvBnAct(\n          (conv): Conv2d(168, 392, kernel_size=(1, 1), stride=(2, 2), bias=False)\n          (bn): BatchNormAct2d(392, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (b2): Bottleneck(\n        (conv1): ConvBnAct(\n          (conv): Conv2d(392, 392, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNormAct2d(\n            392, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n            (act): ReLU(inplace=True)\n          )\n        )\n        (conv2): ConvBnAct(\n          (conv): Conv2d(392, 392, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=7, bias=False)\n          (bn): BatchNormAct2d(\n            392, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n            (act): ReLU(inplace=True)\n          )\n        )\n        (conv3): ConvBnAct(\n          (conv): Conv2d(392, 392, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNormAct2d(392, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (act3): ReLU(inplace=True)\n      )\n      (b3): Bottleneck(\n        (conv1): ConvBnAct(\n          (conv): Conv2d(392, 392, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNormAct2d(\n            392, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n            (act): ReLU(inplace=True)\n          )\n        )\n        (conv2): ConvBnAct(\n          (conv): Conv2d(392, 392, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=7, bias=False)\n          (bn): BatchNormAct2d(\n            392, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n            (act): ReLU(inplace=True)\n          )\n        )\n        (conv3): ConvBnAct(\n          (conv): Conv2d(392, 392, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNormAct2d(392, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (act3): ReLU(inplace=True)\n      )\n      (b4): Bottleneck(\n        (conv1): ConvBnAct(\n          (conv): Conv2d(392, 392, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNormAct2d(\n            392, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n            (act): ReLU(inplace=True)\n          )\n        )\n        (conv2): ConvBnAct(\n          (conv): Conv2d(392, 392, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=7, bias=False)\n          (bn): BatchNormAct2d(\n            392, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n            (act): ReLU(inplace=True)\n          )\n        )\n        (conv3): ConvBnAct(\n          (conv): Conv2d(392, 392, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNormAct2d(392, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (act3): ReLU(inplace=True)\n      )\n    )\n    (s3): RegStage(\n      (b1): Bottleneck(\n        (conv1): ConvBnAct(\n          (conv): Conv2d(392, 784, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNormAct2d(\n            784, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n            (act): ReLU(inplace=True)\n          )\n        )\n        (conv2): ConvBnAct(\n          (conv): Conv2d(784, 784, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=14, bias=False)\n          (bn): BatchNormAct2d(\n            784, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n            (act): ReLU(inplace=True)\n          )\n        )\n        (conv3): ConvBnAct(\n          (conv): Conv2d(784, 784, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNormAct2d(784, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (act3): ReLU(inplace=True)\n        (downsample): ConvBnAct(\n          (conv): Conv2d(392, 784, kernel_size=(1, 1), stride=(2, 2), bias=False)\n          (bn): BatchNormAct2d(784, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (b2): Bottleneck(\n        (conv1): ConvBnAct(\n          (conv): Conv2d(784, 784, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNormAct2d(\n            784, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n            (act): ReLU(inplace=True)\n          )\n        )\n        (conv2): ConvBnAct(\n          (conv): Conv2d(784, 784, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=14, bias=False)\n          (bn): BatchNormAct2d(\n            784, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n            (act): ReLU(inplace=True)\n          )\n        )\n        (conv3): ConvBnAct(\n          (conv): Conv2d(784, 784, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNormAct2d(784, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (act3): ReLU(inplace=True)\n      )\n      (b3): Bottleneck(\n        (conv1): ConvBnAct(\n          (conv): Conv2d(784, 784, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNormAct2d(\n            784, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n            (act): ReLU(inplace=True)\n          )\n        )\n        (conv2): ConvBnAct(\n          (conv): Conv2d(784, 784, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=14, bias=False)\n          (bn): BatchNormAct2d(\n            784, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n            (act): ReLU(inplace=True)\n          )\n        )\n        (conv3): ConvBnAct(\n          (conv): Conv2d(784, 784, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNormAct2d(784, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (act3): ReLU(inplace=True)\n      )\n      (b4): Bottleneck(\n        (conv1): ConvBnAct(\n          (conv): Conv2d(784, 784, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNormAct2d(\n            784, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n            (act): ReLU(inplace=True)\n          )\n        )\n        (conv2): ConvBnAct(\n          (conv): Conv2d(784, 784, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=14, bias=False)\n          (bn): BatchNormAct2d(\n            784, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n            (act): ReLU(inplace=True)\n          )\n        )\n        (conv3): ConvBnAct(\n          (conv): Conv2d(784, 784, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNormAct2d(784, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (act3): ReLU(inplace=True)\n      )\n      (b5): Bottleneck(\n        (conv1): ConvBnAct(\n          (conv): Conv2d(784, 784, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNormAct2d(\n            784, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n            (act): ReLU(inplace=True)\n          )\n        )\n        (conv2): ConvBnAct(\n          (conv): Conv2d(784, 784, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=14, bias=False)\n          (bn): BatchNormAct2d(\n            784, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n            (act): ReLU(inplace=True)\n          )\n        )\n        (conv3): ConvBnAct(\n          (conv): Conv2d(784, 784, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNormAct2d(784, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (act3): ReLU(inplace=True)\n      )\n      (b6): Bottleneck(\n        (conv1): ConvBnAct(\n          (conv): Conv2d(784, 784, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNormAct2d(\n            784, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n            (act): ReLU(inplace=True)\n          )\n        )\n        (conv2): ConvBnAct(\n          (conv): Conv2d(784, 784, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=14, bias=False)\n          (bn): BatchNormAct2d(\n            784, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n            (act): ReLU(inplace=True)\n          )\n        )\n        (conv3): ConvBnAct(\n          (conv): Conv2d(784, 784, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNormAct2d(784, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (act3): ReLU(inplace=True)\n      )\n      (b7): Bottleneck(\n        (conv1): ConvBnAct(\n          (conv): Conv2d(784, 784, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNormAct2d(\n            784, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n            (act): ReLU(inplace=True)\n          )\n        )\n        (conv2): ConvBnAct(\n          (conv): Conv2d(784, 784, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=14, bias=False)\n          (bn): BatchNormAct2d(\n            784, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n            (act): ReLU(inplace=True)\n          )\n        )\n        (conv3): ConvBnAct(\n          (conv): Conv2d(784, 784, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNormAct2d(784, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (act3): ReLU(inplace=True)\n      )\n      (b8): Bottleneck(\n        (conv1): ConvBnAct(\n          (conv): Conv2d(784, 784, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNormAct2d(\n            784, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n            (act): ReLU(inplace=True)\n          )\n        )\n        (conv2): ConvBnAct(\n          (conv): Conv2d(784, 784, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=14, bias=False)\n          (bn): BatchNormAct2d(\n            784, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n            (act): ReLU(inplace=True)\n          )\n        )\n        (conv3): ConvBnAct(\n          (conv): Conv2d(784, 784, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNormAct2d(784, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (act3): ReLU(inplace=True)\n      )\n      (b9): Bottleneck(\n        (conv1): ConvBnAct(\n          (conv): Conv2d(784, 784, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNormAct2d(\n            784, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n            (act): ReLU(inplace=True)\n          )\n        )\n        (conv2): ConvBnAct(\n          (conv): Conv2d(784, 784, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=14, bias=False)\n          (bn): BatchNormAct2d(\n            784, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n            (act): ReLU(inplace=True)\n          )\n        )\n        (conv3): ConvBnAct(\n          (conv): Conv2d(784, 784, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNormAct2d(784, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (act3): ReLU(inplace=True)\n      )\n      (b10): Bottleneck(\n        (conv1): ConvBnAct(\n          (conv): Conv2d(784, 784, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNormAct2d(\n            784, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n            (act): ReLU(inplace=True)\n          )\n        )\n        (conv2): ConvBnAct(\n          (conv): Conv2d(784, 784, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=14, bias=False)\n          (bn): BatchNormAct2d(\n            784, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n            (act): ReLU(inplace=True)\n          )\n        )\n        (conv3): ConvBnAct(\n          (conv): Conv2d(784, 784, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNormAct2d(784, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (act3): ReLU(inplace=True)\n      )\n    )\n    (s4): RegStage(\n      (b1): Bottleneck(\n        (conv1): ConvBnAct(\n          (conv): Conv2d(784, 1624, kernel_size=(1, 1), stride=(1, 1), dilation=(2, 2), bias=False)\n          (bn): BatchNormAct2d(\n            1624, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n            (act): ReLU(inplace=True)\n          )\n        )\n        (conv2): ConvBnAct(\n          (conv): Conv2d(1624, 1624, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=29, bias=False)\n          (bn): BatchNormAct2d(\n            1624, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n            (act): ReLU(inplace=True)\n          )\n        )\n        (conv3): ConvBnAct(\n          (conv): Conv2d(1624, 1624, kernel_size=(1, 1), stride=(1, 1), dilation=(2, 2), bias=False)\n          (bn): BatchNormAct2d(1624, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (act3): ReLU(inplace=True)\n        (downsample): ConvBnAct(\n          (conv): Conv2d(784, 1624, kernel_size=(1, 1), stride=(1, 1), dilation=(2, 2), bias=False)\n          (bn): BatchNormAct2d(1624, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n    )\n  )\n  (decoder): DeepLabV3PlusDecoder(\n    (aspp): Sequential(\n      (0): ASPP(\n        (convs): ModuleList(\n          (0): Sequential(\n            (0): Conv2d(1624, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU()\n          )\n          (1): ASPPSeparableConv(\n            (0): SeparableConv2d(\n              (0): Conv2d(1624, 1624, kernel_size=(3, 3), stride=(1, 1), padding=(12, 12), dilation=(12, 12), groups=1624, bias=False)\n              (1): Conv2d(1624, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            )\n            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU()\n          )\n          (2): ASPPSeparableConv(\n            (0): SeparableConv2d(\n              (0): Conv2d(1624, 1624, kernel_size=(3, 3), stride=(1, 1), padding=(24, 24), dilation=(24, 24), groups=1624, bias=False)\n              (1): Conv2d(1624, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            )\n            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU()\n          )\n          (3): ASPPSeparableConv(\n            (0): SeparableConv2d(\n              (0): Conv2d(1624, 1624, kernel_size=(3, 3), stride=(1, 1), padding=(36, 36), dilation=(36, 36), groups=1624, bias=False)\n              (1): Conv2d(1624, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            )\n            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU()\n          )\n          (4): ASPPPooling(\n            (0): AdaptiveAvgPool2d(output_size=1)\n            (1): Conv2d(1624, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (3): ReLU()\n          )\n        )\n        (project): Sequential(\n          (0): Conv2d(1280, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU()\n          (3): Dropout(p=0.5, inplace=False)\n        )\n      )\n      (1): SeparableConv2d(\n        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n        (1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      )\n      (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (3): ReLU()\n    )\n    (up): UpsamplingBilinear2d(scale_factor=4.0, mode=bilinear)\n    (block1): Sequential(\n      (0): Conv2d(168, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): ReLU()\n    )\n    (block2): Sequential(\n      (0): SeparableConv2d(\n        (0): Conv2d(304, 304, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=304, bias=False)\n        (1): Conv2d(304, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      )\n      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): ReLU()\n    )\n  )\n  (segmentation_head): SegmentationHead(\n    (0): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n    (1): UpsamplingBilinear2d(scale_factor=4.0, mode=bilinear)\n    (2): Activation(\n      (activation): Identity()\n    )\n  )\n) <class 'segmentation_models_pytorch.deeplabv3.model.DeepLabV3Plus'>\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0255575e4ae643369300d2801cc4725e"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "End prediction.\n"
     ]
    }
   ],
   "source": [
    "model_path = saved_path + f'/{model_name}.pt'\n",
    "checkpoint = torch.load(model_path, map_location=device)\n",
    "model.load_state_dict(checkpoint)\n",
    "#model = load_model(model_name, model)\n",
    "make_submission(model, test_loader)"
   ]
  }
 ]
}